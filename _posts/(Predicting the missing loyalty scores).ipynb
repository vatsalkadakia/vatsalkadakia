{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicitng the missing loyalty scores of customers after comapring all 3 regression algorithms we found out that Random Forest is the best of all for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import customers for scoring\n",
    "to_be_scored = pickle.load(open(\"C:/Users/vatsal/Desktop/machine learning/model builidng/data/abc_regression_scoring.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import model and model objects\n",
    "regressor = pickle.load(open(\"C:/Users/vatsal/Desktop/machine learning/model builidng/data/random_forest_regression_model.p\", \"rb\"))\n",
    "one_hot_encoder = pickle.load(open(\"C:/Users/vatsal/Desktop/machine learning/model builidng/data/random_forest_regression_ohe.p\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unused columns\n",
    "\n",
    "to_be_scored.drop([\"customer_id\"], axis =1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "\n",
    "to_be_scored.dropna(how =\"any\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vatsal\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Apply One Hot Encoding\n",
    "\n",
    "categorical_vars = [\"gender\"]\n",
    "encoder_vars_array = one_hot_encoder.transform(to_be_scored[categorical_vars])\n",
    "encoder_feature_names = one_hot_encoder.get_feature_names(categorical_vars)\n",
    "encoder_vars_df = pd.DataFrame(encoder_vars_array, columns = encoder_feature_names)\n",
    "to_be_scored = pd.concat([to_be_scored.reset_index(drop=True), encoder_vars_df.reset_index(drop=True)], axis = 1)\n",
    "to_be_scored.drop(categorical_vars, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_store</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>total_sales</th>\n",
       "      <th>total_items</th>\n",
       "      <th>transaction_count</th>\n",
       "      <th>product_area_count</th>\n",
       "      <th>average_basket_value</th>\n",
       "      <th>gender_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.78</td>\n",
       "      <td>0.66</td>\n",
       "      <td>3980.49</td>\n",
       "      <td>424</td>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "      <td>78.048824</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2887.20</td>\n",
       "      <td>253</td>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>64.160000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>14.91</td>\n",
       "      <td>0.68</td>\n",
       "      <td>3342.75</td>\n",
       "      <td>335</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>71.122340</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.62</td>\n",
       "      <td>2326.71</td>\n",
       "      <td>267</td>\n",
       "      <td>48</td>\n",
       "      <td>5</td>\n",
       "      <td>48.473125</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.58</td>\n",
       "      <td>3448.59</td>\n",
       "      <td>370</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>70.379388</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>458</td>\n",
       "      <td>400.97</td>\n",
       "      <td>0.54</td>\n",
       "      <td>4072.86</td>\n",
       "      <td>346</td>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>90.508000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.38</td>\n",
       "      <td>3097.99</td>\n",
       "      <td>281</td>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "      <td>60.744902</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.63</td>\n",
       "      <td>3736.02</td>\n",
       "      <td>250</td>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>81.217826</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>461</td>\n",
       "      <td>4.36</td>\n",
       "      <td>0.54</td>\n",
       "      <td>4407.12</td>\n",
       "      <td>354</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>93.768511</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>1.87</td>\n",
       "      <td>0.59</td>\n",
       "      <td>3067.83</td>\n",
       "      <td>254</td>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>57.883585</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>463 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     distance_from_store  credit_score  total_sales  total_items  \\\n",
       "0                   4.78          0.66      3980.49          424   \n",
       "1                   3.49          0.38      2887.20          253   \n",
       "2                  14.91          0.68      3342.75          335   \n",
       "3                   0.25          0.62      2326.71          267   \n",
       "4                   4.74          0.58      3448.59          370   \n",
       "..                   ...           ...          ...          ...   \n",
       "458               400.97          0.54      4072.86          346   \n",
       "459                 0.32          0.38      3097.99          281   \n",
       "460                 1.62          0.63      3736.02          250   \n",
       "461                 4.36          0.54      4407.12          354   \n",
       "462                 1.87          0.59      3067.83          254   \n",
       "\n",
       "     transaction_count  product_area_count  average_basket_value  gender_M  \n",
       "0                   51                   5             78.048824       0.0  \n",
       "1                   45                   5             64.160000       0.0  \n",
       "2                   47                   5             71.122340       0.0  \n",
       "3                   48                   5             48.473125       1.0  \n",
       "4                   49                   5             70.379388       0.0  \n",
       "..                 ...                 ...                   ...       ...  \n",
       "458                 45                   5             90.508000       0.0  \n",
       "459                 51                   5             60.744902       1.0  \n",
       "460                 46                   5             81.217826       1.0  \n",
       "461                 47                   5             93.768511       0.0  \n",
       "462                 53                   5             57.883585       1.0  \n",
       "\n",
       "[463 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_be_scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42641, 0.32992, 0.34719, 0.93166, 0.3849 , 0.93566, 0.43343,\n",
       "       0.73502, 0.31208, 0.74406, 0.43255, 0.51648, 0.44943, 0.49396,\n",
       "       0.51378, 0.35195, 0.79852, 0.27749, 0.9132 , 0.39764, 0.28666,\n",
       "       0.57642, 0.45266, 0.30921, 0.6539 , 0.42941, 0.5505 , 0.90092,\n",
       "       0.5734 , 0.32631, 0.35492, 0.41621, 0.47523, 0.56345, 0.67158,\n",
       "       0.34009, 0.33038, 0.89301, 0.36622, 0.44413, 0.85157, 0.72539,\n",
       "       0.45002, 0.93642, 0.94507, 0.22433, 0.39676, 0.96111, 0.53929,\n",
       "       0.44196, 0.8682 , 0.86839, 0.73586, 0.29151, 0.60416, 0.84114,\n",
       "       0.23933, 0.3014 , 0.24201, 0.24305, 0.24934, 0.87329, 0.86826,\n",
       "       0.55587, 0.39386, 0.57611, 0.92211, 0.41034, 0.9446 , 0.25769,\n",
       "       0.61682, 0.45259, 0.64507, 0.49405, 0.62185, 0.39843, 0.16508,\n",
       "       0.61596, 0.7924 , 0.54604, 0.28182, 0.29235, 0.30199, 0.71632,\n",
       "       0.56894, 0.2379 , 0.36931, 0.28792, 0.40425, 0.37513, 0.39393,\n",
       "       0.90329, 0.91231, 0.26916, 0.38641, 0.38063, 0.87808, 0.53341,\n",
       "       0.31401, 0.92022, 0.28421, 0.22333, 0.29671, 0.72241, 0.71109,\n",
       "       0.44478, 0.80592, 0.51689, 0.29636, 0.85891, 0.93109, 0.29279,\n",
       "       0.27718, 0.37517, 0.24839, 0.24201, 0.21925, 0.51374, 0.28497,\n",
       "       0.88146, 0.63028, 0.2433 , 0.33895, 0.86955, 0.62025, 0.45636,\n",
       "       0.47857, 0.74885, 0.20443, 0.42924, 0.24584, 0.53652, 0.39955,\n",
       "       0.73547, 0.93121, 0.76077, 0.29743, 0.85079, 0.64271, 0.1637 ,\n",
       "       0.47868, 0.18868, 0.5167 , 0.82551, 0.80383, 0.50916, 0.67587,\n",
       "       0.17159, 0.40686, 0.538  , 0.40701, 0.36032, 0.34631, 0.19037,\n",
       "       0.29645, 0.54229, 0.77591, 0.22881, 0.56545, 0.80365, 0.10114,\n",
       "       0.09471, 0.09955, 0.14688, 0.09349, 0.19394, 0.25337, 0.29031,\n",
       "       0.36958, 0.36677, 0.38093, 0.79082, 0.20437, 0.77797, 0.15015,\n",
       "       0.20907, 0.34256, 0.17533, 0.88382, 0.10741, 0.09883, 0.22342,\n",
       "       0.90502, 0.54847, 0.87578, 0.55508, 0.89142, 0.56908, 0.57058,\n",
       "       0.81966, 0.78845, 0.74783, 0.26777, 0.50547, 0.44314, 0.27999,\n",
       "       0.20889, 0.16085, 0.45665, 0.85375, 0.46467, 0.24278, 0.45143,\n",
       "       0.14246, 0.80031, 0.4471 , 0.86823, 0.16405, 0.10665, 0.73568,\n",
       "       0.11098, 0.20651, 0.55887, 0.38577, 0.77804, 0.70519, 0.62433,\n",
       "       0.89597, 0.7548 , 0.74926, 0.65525, 0.73601, 0.70541, 0.83293,\n",
       "       0.83501, 0.72453, 0.79258, 0.77518, 0.77704, 0.75239, 0.8344 ,\n",
       "       0.88867, 0.66647, 0.69458, 0.75838, 0.77232, 0.77505, 0.84193,\n",
       "       0.57867, 0.72896, 0.76661, 0.80051, 0.89353, 0.82548, 0.90156,\n",
       "       0.78061, 0.69444, 0.82633, 0.77937, 0.89444, 0.70504, 0.78151,\n",
       "       0.78572, 0.76261, 0.79878, 0.82676, 0.60511, 0.77192, 0.89106,\n",
       "       0.79409, 0.8619 , 0.79748, 0.81815, 0.86553, 0.75754, 0.88753,\n",
       "       0.74557, 0.38952, 0.73299, 0.46052, 0.47823, 0.82325, 0.41464,\n",
       "       0.56388, 0.44853, 0.39471, 0.57038, 0.45675, 0.84018, 0.51212,\n",
       "       0.51354, 0.67535, 0.52224, 0.58184, 0.68572, 0.59521, 0.60686,\n",
       "       0.42119, 0.79707, 0.479  , 0.86083, 0.36057, 0.84164, 0.36619,\n",
       "       0.79337, 0.36109, 0.90282, 0.43473, 0.33075, 0.43869, 0.51379,\n",
       "       0.50624, 0.45304, 0.40159, 0.58863, 0.78923, 0.80982, 0.38097,\n",
       "       0.80358, 0.56577, 0.35922, 0.84128, 0.68694, 0.76794, 0.66276,\n",
       "       0.75062, 0.46246, 0.72962, 0.40661, 0.36385, 0.17785, 0.07135,\n",
       "       0.52469, 0.49615, 0.05388, 0.33145, 0.12559, 0.31739, 0.03712,\n",
       "       0.45152, 0.14704, 0.05043, 0.11267, 0.39101, 0.41657, 0.31623,\n",
       "       0.38067, 0.02059, 0.2717 , 0.38581, 0.15604, 0.06667, 0.72873,\n",
       "       0.17917, 0.18208, 0.116  , 0.03068, 0.03247, 0.1795 , 0.29188,\n",
       "       0.16501, 0.028  , 0.25335, 0.76132, 0.10452, 0.01109, 0.05599,\n",
       "       0.6364 , 0.04439, 0.54421, 0.72229, 0.1828 , 0.04143, 0.04596,\n",
       "       0.30211, 0.65218, 0.6522 , 0.06261, 0.06702, 0.55115, 0.26592,\n",
       "       0.04065, 0.38072, 0.14814, 0.25214, 0.40083, 0.06718, 0.04978,\n",
       "       0.27081, 0.763  , 0.27274, 0.01991, 0.20738, 0.15419, 0.03173,\n",
       "       0.4583 , 0.02311, 0.07049, 0.25695, 0.25517, 0.74189, 0.53389,\n",
       "       0.05167, 0.21553, 0.03542, 0.09117, 0.31496, 0.12871, 0.04534,\n",
       "       0.50676, 0.058  , 0.25705, 0.63532, 0.04603, 0.56068, 0.04244,\n",
       "       0.35448, 0.05087, 0.0849 , 0.07654, 0.29115, 0.17515, 0.1729 ,\n",
       "       0.44585, 0.07481, 0.24778, 0.39697, 0.01679, 0.01172, 0.03086,\n",
       "       0.08979, 0.52285, 0.52424, 0.51683, 0.6811 , 0.77374, 0.52222,\n",
       "       0.98309, 0.68777, 0.9142 , 0.52805, 0.5443 , 0.72888, 0.91056,\n",
       "       0.54519, 0.51669, 0.61829, 0.55545, 0.91795, 0.86077, 0.77379,\n",
       "       0.71516, 0.35747, 0.67392, 0.4591 , 0.90351, 0.65106, 0.93858,\n",
       "       0.46215, 0.82747, 0.9599 , 0.67425, 0.58229, 0.41303, 0.61726,\n",
       "       0.89449, 0.95936, 0.70956, 0.36794, 0.92925, 0.6732 , 0.36973,\n",
       "       0.61284])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make our predictions\n",
    "\n",
    "loyalty_predictions = regressor.predict(to_be_scored)\n",
    "loyalty_predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
